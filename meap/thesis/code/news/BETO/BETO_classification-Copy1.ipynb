{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e6cdc4df-0d2e-4f39-85c6-134dbc2107ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "from transformers import  BertTokenizer\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dd937ff-187f-4cb0-b03b-01986bedce0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select cpu or cuda\n",
    "run_on = 'cpu'\n",
    "device = torch.device(run_on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abf97a0e-1a8d-4281-a930-767a5dcd94c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the data:\n",
    "df = pd.read_csv('/Users/mario/OneDrive/ITAM/4to semestre/Inv Aplicada II/Mario - Tesis/code/BETO/data/sample_results_text_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f76ea03e-7b12-45b4-9cf5-af8959f1e05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset into a pandas dataframe.\n",
    "reviews = df['text']\n",
    "sentiment = df['label'] # 1: Pro // 0: Anti "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85eb8463-7bd7-496c-a651-aae4242d224b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "X_train, X_val, y_train, y_val = train_test_split(reviews, sentiment, stratify=sentiment, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08a8cf70-fcab-4f7e-8fb1-bbb57c9c7bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length : 14\n",
      "Validation set length : 6\n"
     ]
    }
   ],
   "source": [
    "# Report datasets lenghts\n",
    "print('Training set length : {}'.format(len(X_train)))\n",
    "print('Validation set length : {}'.format(len(X_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3590f04-bf71-47f4-9f30-8b19f68ef956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "tokenizer = BertTokenizer.from_pretrained(\"../pytorch/\", do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b58f779f-d91c-4641-be20-895db2c42d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(dataset):\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    for doc in dataset:\n",
    "        encoded_doc = tokenizer.encode_plus(doc,\n",
    "                   add_special_tokens=True, max_length=512,\n",
    "                   truncation=True,padding=True)\n",
    "        input_ids.append(encoded_doc['input_ids'])\n",
    "        attention_mask.append(encoded_doc['attention_mask'])\n",
    "    return (torch.tensor(input_ids),\n",
    "           torch.tensor(attention_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5d51229-3a23-4750-b8ee-37f414eab1cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'periodico la jornadamiercoles de enero de p la bancada de morena en la camara de diputados reformara el articulo bis de la ley general de responsabilidades administrativas para catalogar como grave que las dependencias federales cometan subejercicios carol altamirano representante por ese partido puntualizo que si bien la ley federal de presupuesto y responsabilidad hacendaria indica que el subejercicio sera sancionado conforme a la nueva ley general de responsabilidades al revisar las faltas contenidas en esa ley no hay ninguna que describa con precision el subejercicio la iniciativa ya se encuentra en estudio para su dictamen en la comision de transparencia y anticorrupcion en su contenido se subraya que si un funcionario cae en el subejercicio se expondra a una sancion administrativa y al ser una conducta grave podra ameritar la suspension destitucion del empleo sancion economica e inhabilitacion temporal en el servicio publico el diputado morenista senalo que los subejercicios afectan directamente la aplicacion de los programas sociales y por ende se dejan de atender las necesidades sociales a pesar de contar con el dinero ya presupuestado para hacerlo en el centro de estudios de las finanzas publicas detecto un subejercicio por mil millones de pesos esto impacto a programas que debieran beneficiar a la gente en agricultura se dejaron de ejercer mas de mil millones de pesos en comunicaciones y transportes se registro un subejercicio de mas de mil millones mientras que en salud fue por mil millones y en educacion mas de mil millones de pesos eso es intolerable un funcionario que incumple deliberadamente las metas presupuestarias es un inepto que no debe estar en la administracion publica'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d7c2521-753b-4e94-8073-58a6e7cde19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mario\\anaconda3\\envs\\InvAplicada2\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing to dataset\n",
    "X_train_inputs, X_train_masks = preprocessing(X_train)\n",
    "X_val_inputs, X_val_masks = preprocessing(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21dd17cf-b845-4ec3-b09a-d396547f6506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    4,  1247,  9478,  1081,  8727,  1086,  1294, 30984,  3285,  2436,\n",
       "         1041,  1035,  1069,  2022,  1054,  6612,  1137, 11418,  2450,  4418,\n",
       "        11418,  1600,  5023,  1039,  1939,  7085,  1009,  3050,  1009,  1853,\n",
       "         1039,  2914,  1009, 10877,  4741,  2645, 14579, 30956,  1096,  1195,\n",
       "         1853,  1067,  8814,  1054,  4418, 11418,  1600,  1151,  1028,  1250,\n",
       "         1096,  1652,  1247,  1012, 11053, 11418,  1057,  3148,  1946,  1009,\n",
       "         1041,  1067,  4661, 26461,  1320,  2788,  1048,  2988,  1074,  7085,\n",
       "         3229,  1905, 12534,  1035,  1032, 18288,  1332,  2213,  1018,  1009,\n",
       "         1329,  1190, 30991,  1035, 12210,  1081,  2248, 25958, 30957,  1283,\n",
       "         2310,  1022,  1009,  1032,  3457,  1009,  1032,  8808, 30956,  1040,\n",
       "         1032, 18081,  1054,  1057, 17887, 30957, 21843, 15133,  1339,  1040,\n",
       "        24034,  1039, 30827,  1035,     5])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_inputs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c03ce73f-1960-4605-a5ac-de5be2e847dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max n°tokens in a sentence: 115\n"
     ]
    }
   ],
   "source": [
    "# Report max n° tokens in a sentence\n",
    "max_len = max([torch.sum(sen) for sen in X_train_masks])\n",
    "print('Max n°tokens in a sentence: {0}'.format(max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55e0857b-e197-42f3-82cc-ac46e68509ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loaders\n",
    "batch_size = 64\n",
    "y_train_labels = torch.tensor(y_train.values)\n",
    "y_val_labels = torch.tensor(y_val.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f2d9c56-3adb-4ba9-ad06-98d10a7f72ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(x_inputs, x_masks, y_labels):\n",
    "    data = TensorDataset(x_inputs, x_masks, y_labels)\n",
    "    sampler = SequentialSampler(data)\n",
    "    dataloader = DataLoader(data, sampler=sampler,\n",
    "                 batch_size=batch_size,\n",
    "                 num_workers=4)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "262ec96a-f201-4511-ab61-03b66353cdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = dataloader(X_train_inputs, X_train_masks,\n",
    "                   y_train_labels)\n",
    "val_dataloader = dataloader(X_val_inputs, X_val_masks, \n",
    "                 y_val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f6c52ef3-fc8b-4d8f-bd57-cde3f52e833f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "import torch\n",
    "\n",
    "# set random seed\n",
    "def set_seed(value):\n",
    "    random.seed(value)\n",
    "    np.random.seed(value)\n",
    "    torch.manual_seed(value)\n",
    "    torch.cuda.manual_seed_all(value)\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e8b51772-4e98-439a-8ae5-3ac7047ad43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../pytorch/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "model = BertForSequenceClassification.from_pretrained(\"../pytorch/\", num_labels=2, output_attentions=False, output_hidden_states=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b5408dec-4eaa-42b9-8886-fc9a3f3b8013",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.softmax = torch.nn.Softmax(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5ecb2b06-b62c-40ee-bcd1-4c6db978f40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mario\\anaconda3\\envs\\InvAplicada2\\lib\\site-packages\\transformers\\optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 4e-5,\n",
    "                  eps = 1e-6\n",
    "                  )\n",
    "\n",
    "if run_on == 'cuda':\n",
    "    model.cuda()\n",
    "\n",
    "# Define number of epochs\n",
    "epochs = 10\n",
    "\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "            num_warmup_steps = 0, \n",
    "            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "db415477-ab41-47fc-89ec-bb6794dad142",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fuction to format time\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "#function to compute accuracy\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "59850e7f-d17e-4ce1-a1b6-ef04a8c10e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Training =======\n",
      "\n",
      "======= Epoch 1 / 10 =======\n",
      "batch loss: 0.09645400941371918 | avg loss: 0.09645400941371918\n",
      "\n",
      "  Average training loss: 0.10\n",
      "  Training epoch took: 0:00:06\n",
      "\n",
      "======= Validation =======\n",
      "  Accuracy: 0.67\n",
      "  Validation took: 0:00:01\n",
      "\n",
      "======= Epoch 2 / 10 =======\n",
      "batch loss: 0.0942695215344429 | avg loss: 0.0942695215344429\n",
      "\n",
      "  Average training loss: 0.09\n",
      "  Training epoch took: 0:00:06\n",
      "\n",
      "======= Validation =======\n",
      "  Accuracy: 0.67\n",
      "  Validation took: 0:00:01\n",
      "\n",
      "======= Epoch 3 / 10 =======\n",
      "batch loss: 0.09492313116788864 | avg loss: 0.09492313116788864\n",
      "\n",
      "  Average training loss: 0.09\n",
      "  Training epoch took: 0:00:06\n",
      "\n",
      "======= Validation =======\n",
      "  Accuracy: 0.67\n",
      "  Validation took: 0:00:01\n",
      "\n",
      "======= Epoch 4 / 10 =======\n",
      "batch loss: 0.08322443813085556 | avg loss: 0.08322443813085556\n",
      "\n",
      "  Average training loss: 0.08\n",
      "  Training epoch took: 0:00:06\n",
      "\n",
      "======= Validation =======\n",
      "  Accuracy: 0.67\n",
      "  Validation took: 0:00:01\n",
      "\n",
      "======= Epoch 5 / 10 =======\n",
      "batch loss: 0.10232424736022949 | avg loss: 0.10232424736022949\n",
      "\n",
      "  Average training loss: 0.10\n",
      "  Training epoch took: 0:00:06\n",
      "\n",
      "======= Validation =======\n",
      "  Accuracy: 0.67\n",
      "  Validation took: 0:00:01\n",
      "\n",
      "======= Epoch 6 / 10 =======\n",
      "batch loss: 0.09246470779180527 | avg loss: 0.09246470779180527\n",
      "\n",
      "  Average training loss: 0.09\n",
      "  Training epoch took: 0:00:06\n",
      "\n",
      "======= Validation =======\n",
      "  Accuracy: 0.67\n",
      "  Validation took: 0:00:01\n",
      "\n",
      "======= Epoch 7 / 10 =======\n",
      "batch loss: 0.08810848742723465 | avg loss: 0.08810848742723465\n",
      "\n",
      "  Average training loss: 0.09\n",
      "  Training epoch took: 0:00:06\n",
      "\n",
      "======= Validation =======\n",
      "  Accuracy: 0.67\n",
      "  Validation took: 0:00:01\n",
      "\n",
      "======= Epoch 8 / 10 =======\n",
      "batch loss: 0.09709759056568146 | avg loss: 0.09709759056568146\n",
      "\n",
      "  Average training loss: 0.10\n",
      "  Training epoch took: 0:00:06\n",
      "\n",
      "======= Validation =======\n",
      "  Accuracy: 0.67\n",
      "  Validation took: 0:00:01\n",
      "\n",
      "======= Epoch 9 / 10 =======\n",
      "batch loss: 0.09133316576480865 | avg loss: 0.09133316576480865\n",
      "\n",
      "  Average training loss: 0.09\n",
      "  Training epoch took: 0:00:06\n",
      "\n",
      "======= Validation =======\n",
      "  Accuracy: 0.67\n",
      "  Validation took: 0:00:01\n",
      "\n",
      "======= Epoch 10 / 10 =======\n",
      "batch loss: 0.08930297195911407 | avg loss: 0.08930297195911407\n",
      "\n",
      "  Average training loss: 0.09\n",
      "  Training epoch took: 0:00:06\n",
      "\n",
      "======= Validation =======\n",
      "  Accuracy: 0.67\n",
      "  Validation took: 0:00:01\n",
      "[[1.         0.        ]\n",
      " [0.66666667 0.33333333]]\n",
      "\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "#function to train the model\n",
    "def training(n_epochs, training_dataloader,\n",
    "             validation_dataloader):\n",
    "    # ========================================\n",
    "    #               Training \n",
    "    # ========================================\n",
    "    print('======= Training =======')\n",
    "    for epoch_i in range(0,n_epochs):\n",
    "        # Perform one full pass over the training set\n",
    "        print(\"\")\n",
    "        print('======= Epoch {:} / {:} ======='.format(\n",
    "             epoch_i + 1, epochs))\n",
    "        # Measure how long the training epoch takes.\n",
    "        t0 = time.time()\n",
    "        # Reset the total loss for this epoch.\n",
    "        total_loss = 0\n",
    "        # Put the model into training mode.\n",
    "        model.train()\n",
    "        # For each batch of training data\n",
    "        for step, batch in enumerate(training_dataloader):\n",
    "            batch_loss = 0\n",
    "            # Unpack this training batch from dataloader\n",
    "            #   [0]: input ids, [1]: attention masks, \n",
    "            #   [2]: labels\n",
    "            b_input_ids,b_input_mask, b_labels = tuple(\n",
    "                                t.to(device) for t in batch)\n",
    "\n",
    "            # Clear any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Perform a forward pass \n",
    "            outputs = model(b_input_ids,\n",
    "                            token_type_ids=None,\n",
    "                            attention_mask=b_input_mask,\n",
    "                            labels=b_labels)\n",
    "\n",
    "            # pull loss value out of the output tuple\n",
    "            loss = outputs[0]\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass \n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(),\n",
    "                                            1.0)\n",
    "\n",
    "            # Update parameters\n",
    "            # ¿take a step using the computed gradient\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            print('batch loss: {0} | avg loss: {1}'.format(\n",
    "                  batch_loss, total_loss/(step+1)))\n",
    "        # Calculate the average loss over the training data.\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"  Average training loss: {0:.2f}\".\n",
    "             format(avg_train_loss))\n",
    "        print(\"  Training epoch took: {:}\".format(\n",
    "              format_time(time.time() - t0)))\n",
    "\n",
    "        # ========================================\n",
    "        #               Validation\n",
    "        # ========================================\n",
    "        # After the completion of each training epoch, \n",
    "        # measure accuracy on the validation set.\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"======= Validation =======\")\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Put the model in evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # Tracking variables\n",
    "        eval_loss, eval_accuracy = 0, 0\n",
    "        all_logits = []\n",
    "        all_labels = []\n",
    "        # Evaluate data for one epoch\n",
    "        for step, batch in enumerate(validation_dataloader):\n",
    "            # Add batch to device\n",
    "            # Unpack this training batch from our dataloader.\n",
    "            #   [0]: input ids, [1]: attention masks,\n",
    "            #   [2]: labels\n",
    "            b_input_ids, b_input_mask, b_labels = tuple(\n",
    "                                t.to(device) for t in batch)\n",
    "\n",
    "\n",
    "            # Model will not to compute gradients\n",
    "            with torch.no_grad():\n",
    "                # Forward pass \n",
    "                # This will return the logits \n",
    "                outputs = model(b_input_ids,\n",
    "                                token_type_ids=None,\n",
    "                                attention_mask=b_input_mask)\n",
    "\n",
    "            # The \"logits\" are the output values \n",
    "            # prior to applying an activation function \n",
    "            logits = outputs[0]\n",
    "\n",
    "            # Move logits and labels to CPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            b_labels = b_labels.to('cpu').numpy()\n",
    "\n",
    "            # Save batch logits and labels \n",
    "            # We will use thoses in the confusion matrix\n",
    "            predict_labels = np.argmax(\n",
    "                             logits, axis=1).flatten()\n",
    "            all_logits.extend(predict_labels.tolist())\n",
    "            all_labels.extend(b_labels.tolist())\n",
    "\n",
    "            # Calculate the accuracy for this batch\n",
    "            tmp_eval_accuracy = flat_accuracy(\n",
    "                                logits, b_labels)\n",
    "            # Accumulate the total accuracy.\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        # Report the final accuracy for this validation run.\n",
    "        print(\"  Accuracy: {0:.2f}\".\n",
    "              format(eval_accuracy / (step+1)))\n",
    "        print(\"  Validation took: {:}\".format(\n",
    "             format_time(time.time() - t0)))\n",
    "\n",
    "    #print the confusion matrix\"\n",
    "    conf = confusion_matrix(\n",
    "           all_labels, all_logits, normalize='true')\n",
    "    print(conf)\n",
    "    print(\"\")\n",
    "    print(\"Training complete\")\n",
    "\n",
    "#call function to train the model\n",
    "training(epochs, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e302c4-fc19-48e6-aa4c-4c0c44a13509",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
